import time
import requests
import json
import argparse
from datasets import load_dataset, Dataset, DatasetDict
from typing import Dict, Any, List
from transformers import AutoTokenizer  # âœ… æ–°å¢ï¼šç”¨äºåŠ è½½ tokenizer å’Œåº”ç”¨ chat template

# --- é…ç½®å‚æ•° ---
URL = "http://localhost:8088/generate"
HEADERS = {"Content-Type": "application/json"}
MAX_NEW_TOKENS = 256 
CACHE_DIR = "./hf_datasets_cache"
DEFAULT_SPLIT = 'test'

# âœ… åŠ è½½ DeepSeek-R1 çš„ tokenizerï¼ˆå‡è®¾æ¨¡å‹åä¸º "deepseek-ai/deepseek-r1"ï¼‰
# æ³¨æ„ï¼šä½ éœ€è¦ç¡®ä¿æœ¬åœ°æˆ– Hugging Face ä¸Šæœ‰è¿™ä¸ªæ¨¡å‹çš„ tokenizer
TOKENIZER_NAME = "/data/nvme1/models/DeepSeek-R1"
print(f"ğŸ“¥ Loading tokenizer for '{TOKENIZER_NAME}'...")
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, trust_remote_code=True)

# --- æ•°æ®é›†æ˜ å°„é…ç½® ---
DATASET_CONFIGS = {
    "mt-bench": {
        "hf_name": "HuggingFaceH4/mt_bench_prompts",
        "split": "train", 
        "prompt_key": "prompt",
        "combine_func": lambda item: item[0],
        "use_chat_template": True,
        "system_prompt": "You are a helpful AI assistant named DeepSeek."
    },

    "gsm8k": {
        "hf_name": "openai/gsm8k", 
        "hf_config": "main", 
        "split": DEFAULT_SPLIT,
        "prompt_key": "question",
        "use_chat_template": True,
        "system_prompt": "You are a helpful AI assistant named DeepSeek."
    },
    
    "humaneval": {
        "hf_name": "openai_humaneval", 
        "split": DEFAULT_SPLIT,
        "prompt_key": "prompt",
        "use_chat_template": True,
        "system_prompt": "You are a helpful AI assistant named DeepSeek. Please complete the Python function below."
    },
    
    "alpaca": {
        "hf_name": "yahma/alpaca-cleaned", 
        "split": "train",
        "prompt_keys": ["instruction", "input"],
        "combine_func": lambda item: (
            f"Instruction: {item['instruction']}\\nInput: {item['input']}"
            if item['input'] else item['instruction']
        ),
        "use_chat_template": True,
        "system_prompt": "You are a helpful AI assistant named DeepSeek."
    },
    
    "cnn_dm": {
        "hf_name": "abisee/cnn_dailymail", 
        "hf_config": "3.0.0", 
        "split": DEFAULT_SPLIT,
        "prompt_key": "article",
        "use_chat_template": True,
        "system_prompt": "You are a helpful AI assistant named DeepSeek. Summarize the following article."
    },
}

# --- è¾…åŠ©å‡½æ•°ï¼šæ•°æ®åŠ è½½å’Œæå– ---
def load_task_dataset(task_name: str) -> Dataset:
    """æ ¹æ®ä»»åŠ¡åç§°åŠ è½½ Hugging Face Dataset å¹¶è¿”å›ç”¨äºæµ‹è¯•çš„ Dataset å¯¹è±¡ã€‚"""
    if task_name not in DATASET_CONFIGS:
        raise ValueError(f"Unknown task name: {task_name}. Supported tasks: {list(DATASET_CONFIGS.keys())}")

    config = DATASET_CONFIGS[task_name]
    hf_name = config['hf_name']
    hf_config = config.get('hf_config')
    split = config['split']
    
    print(f"ğŸ’¡ Loading task '{task_name}' from '{hf_name}' (Split: {split})...")
    
    if task_name == "mt-bench":
        data = load_dataset(hf_name, split=None, cache_dir=CACHE_DIR)
        if isinstance(data, DatasetDict):
             return data[list(data.keys())[0]] 
        return data
        
    dataset = load_dataset(hf_name, hf_config, split=split, cache_dir=CACHE_DIR)
    return dataset

def extract_prompt(item: Dict[str, Any], config: Dict[str, Any]) -> str:
    """æ ¹æ®æ•°æ®é›†é…ç½®ä»å•ä¸ªæ•°æ®é¡¹ä¸­æå–æˆ–æ„é€ æœ€ç»ˆçš„ Prompt å­—ç¬¦ä¸²ï¼Œå¹¶åº”ç”¨ chat templateï¼ˆå¦‚å¯ç”¨ï¼‰ã€‚"""
    prompt_key = config['prompt_key']
    if 'combine_func' in config:
        raw_prompt = config['combine_func'](item[prompt_key])
    else:
        raw_prompt = item[prompt_key]

    if config.get('use_chat_template', False):
        system_prompt = config.get('system_prompt', "")
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": raw_prompt})

        # âœ… ä½¿ç”¨ tokenizer.apply_chat_template è‡ªåŠ¨æ ¼å¼åŒ–
        templated_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,      # è¿”å›å­—ç¬¦ä¸²è€Œé token IDs
            add_generation_prompt=True  # æ·»åŠ  Assistant: å‰ç¼€ï¼Œå‡†å¤‡ç”Ÿæˆ
        )
        return templated_prompt

    return raw_prompt

# --- ä¸»è¿è¡Œé€»è¾‘ï¼šé¡ºåºå‘é€è¯·æ±‚ ---
def run_benchmark(task_name: str):
    try:
        dataset = load_task_dataset(task_name)
        config = DATASET_CONFIGS[task_name]
    except Exception as e:
        print(f"ğŸš¨ Failed to load dataset for task {task_name}: {e}")
        return

    print(f"\nğŸš€ Starting sequential inference test on {task_name} with {len(dataset)} examples...")
    print("ğŸ“¢ Testing started. MTP metrics logging is assumed to be handled by the server.")

    for i, item in enumerate(dataset):
        try:
            prompt = extract_prompt(item, config)
        except Exception as e:
            print(f"âš ï¸ Skipping item {i} (QID: {item.get('question_id', 'N/A')}) due to prompt extraction error: {e}")
            continue
        print(prompt)

        api_data = {
            "inputs": prompt,
            "parameters": {
                "do_sample": False,
                "max_new_tokens": MAX_NEW_TOKENS,
            },
            # # ä¼ é€’å…ƒæ•°æ®åˆ°æœåŠ¡ç«¯ï¼Œä»¥ä¾¿æœåŠ¡ç«¯å°† MTP æŒ‡æ ‡ä¸è¯·æ±‚ ID å…³è”
            # "request_metadata": {
            #     "task_name": task_name,
            #     "item_index": i,
            #     "question_id": item.get('question_id', i),
            #     "prompt_length": len(prompt) # ç²—ç•¥çš„å­—ç¬¦é•¿åº¦
            # } 
        }
        
        start_time = time.time()
        try:
            # å‘é€è¯·æ±‚å¹¶ç­‰å¾…å“åº”ï¼ˆåŒæ­¥ï¼‰
            response = requests.post(URL, headers=HEADERS, data=json.dumps(api_data), timeout=180) 
            e2e_latency = time.time() - start_time
            
            if response.status_code == 200:
                print(f"[{i+1}/{len(dataset)}] âœ… Task: {task_name} | Latency: {e2e_latency:.2f}s")
            else:
                print(f"[{i+1}/{len(dataset)}] âŒ Task: {task_name} | Status: {response.status_code}")

        except requests.exceptions.RequestException as e:
            print(f"[{i+1}/{len(dataset)}] ğŸš¨ Task: {task_name} | Request Failed: {e}")

    print("\nâœ… All requests finished for the current task.")


# --- å‘½ä»¤è¡Œè§£æ ---
def main():
    parser = argparse.ArgumentParser(description="Run LLM MTP benchmark on specified HuggingFace dataset.")
    parser.add_argument(
        '--task_name',
        type=str,
        required=True,
        choices=list(DATASET_CONFIGS.keys()),
        help=f"The benchmark task name to run. Choices: {list(DATASET_CONFIGS.keys())}"
    )
    args = parser.parse_args()
    
    run_benchmark(args.task_name)


if __name__ == "__main__":
    main()